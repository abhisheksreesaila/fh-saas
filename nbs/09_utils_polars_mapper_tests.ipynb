{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sqlalchemy import create_engine, text\n",
    "from fh_saas.utils_polars_mapper import map_and_upsert, apply_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cdb785",
   "metadata": {},
   "source": [
    "## Test Upsert with Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c627e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test passed: Map and upsert (INSERT)\n"
     ]
    }
   ],
   "source": [
    "def test_map_and_upsert_insert():\n",
    "    \"\"\"Test staging table upsert - INSERT new rows\"\"\"\n",
    "    \n",
    "    # Create in-memory SQLite database (shared mode so multiple connections see the same DB)\n",
    "    db_uri = 'sqlite:///file:memdb1?mode=memory&cache=shared&uri=true'\n",
    "    engine = create_engine(db_uri)\n",
    "    \n",
    "    # Create target table (drop first to ensure clean state)\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS users\"))\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE users (\n",
    "                user_id INTEGER PRIMARY KEY,\n",
    "                name TEXT,\n",
    "                email TEXT\n",
    "            )\n",
    "        \"\"\"))\n",
    "        conn.commit()\n",
    "    \n",
    "    # Prepare test data\n",
    "    json_data = [\n",
    "        {'user_id_val': 1, 'ABC_1': 'Alice', 'email_addr': 'alice@example.com', 'extra': 'ignore'},\n",
    "        {'user_id_val': 2, 'ABC_1': 'Bob', 'email_addr': 'bob@example.com', 'extra': 'ignore'}\n",
    "    ]\n",
    "    df = pl.DataFrame(json_data)\n",
    "    \n",
    "    # Execute upsert\n",
    "    map_and_upsert(\n",
    "        df=df,\n",
    "        table_name='users',\n",
    "        key_col='user_id',\n",
    "        db_uri=db_uri,\n",
    "        column_map={\n",
    "            'user_id_val': 'user_id',\n",
    "            'ABC_1': 'name',\n",
    "            'email_addr': 'email'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Verify results\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT * FROM users ORDER BY user_id\"))\n",
    "        rows = result.fetchall()\n",
    "        \n",
    "        assert len(rows) == 2\n",
    "        assert rows[0] == (1, 'Alice', 'alice@example.com')\n",
    "        assert rows[1] == (2, 'Bob', 'bob@example.com')\n",
    "    \n",
    "    print(\"✅ Test passed: Map and upsert (INSERT)\")\n",
    "\n",
    "test_map_and_upsert_insert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617bab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test passed: Map and upsert (UPDATE)\n"
     ]
    }
   ],
   "source": [
    "def test_map_and_upsert_update():\n",
    "    \"\"\"Test staging table upsert - UPDATE existing rows\"\"\"\n",
    "    \n",
    "    # Create in-memory SQLite database (shared mode so multiple connections see the same DB)\n",
    "    db_uri = 'sqlite:///file:memdb2?mode=memory&cache=shared&uri=true'\n",
    "    engine = create_engine(db_uri)\n",
    "    \n",
    "    # Create target table with existing data (drop first to ensure clean state)\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS users\"))\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE users (\n",
    "                user_id INTEGER PRIMARY KEY,\n",
    "                name TEXT,\n",
    "                email TEXT\n",
    "            )\n",
    "        \"\"\"))\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO users (user_id, name, email) VALUES\n",
    "            (1, 'Alice OLD', 'alice_old@example.com'),\n",
    "            (2, 'Bob OLD', 'bob_old@example.com')\n",
    "        \"\"\"))\n",
    "        conn.commit()\n",
    "    \n",
    "    # Prepare updated data\n",
    "    json_data = [\n",
    "        {'user_id': 1, 'name': 'Alice NEW', 'email': 'alice_new@example.com'},\n",
    "        {'user_id': 2, 'name': 'Bob NEW', 'email': 'bob_new@example.com'}\n",
    "    ]\n",
    "    df = pl.DataFrame(json_data)\n",
    "    \n",
    "    # Execute upsert (should UPDATE existing rows)\n",
    "    map_and_upsert(\n",
    "        df=df,\n",
    "        table_name='users',\n",
    "        key_col='user_id',\n",
    "        db_uri=db_uri\n",
    "    )\n",
    "    \n",
    "    # Verify rows were UPDATED (not duplicated)\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT * FROM users ORDER BY user_id\"))\n",
    "        rows = result.fetchall()\n",
    "        \n",
    "        assert len(rows) == 2  # Still only 2 rows (no duplicates)\n",
    "        assert rows[0] == (1, 'Alice NEW', 'alice_new@example.com')\n",
    "        assert rows[1] == (2, 'Bob NEW', 'bob_new@example.com')\n",
    "    \n",
    "    print(\"✅ Test passed: Map and upsert (UPDATE)\")\n",
    "\n",
    "test_map_and_upsert_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12072f",
   "metadata": {},
   "source": [
    "## Test Schema Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test passed: Apply schema (Date)\n"
     ]
    }
   ],
   "source": [
    "def test_apply_schema_date():\n",
    "    \"\"\"Test date string conversion\"\"\"\n",
    "    \n",
    "    df = pl.DataFrame({\n",
    "        'created_at': ['2024-01-15', '2024-01-16']\n",
    "    })\n",
    "    \n",
    "    df = apply_schema(df, {'created_at': pl.Date})\n",
    "    \n",
    "    assert df.schema['created_at'] == pl.Date\n",
    "    print(\"✅ Test passed: Apply schema (Date)\")\n",
    "\n",
    "test_apply_schema_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae87b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test passed: Apply schema (Boolean)\n"
     ]
    }
   ],
   "source": [
    "def test_apply_schema_boolean():\n",
    "    \"\"\"Test boolean string conversion\"\"\"\n",
    "    \n",
    "    df = pl.DataFrame({\n",
    "        'is_active': ['true', 'false', 'True', 'False']\n",
    "    })\n",
    "    \n",
    "    df = apply_schema(df, {'is_active': pl.Boolean})\n",
    "    \n",
    "    assert df.schema['is_active'] == pl.Boolean\n",
    "    assert df['is_active'].to_list() == [True, False, True, False]\n",
    "    print(\"✅ Test passed: Apply schema (Boolean)\")\n",
    "\n",
    "test_apply_schema_boolean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df1179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test passed: Apply schema (Numeric)\n"
     ]
    }
   ],
   "source": [
    "def test_apply_schema_numeric():\n",
    "    \"\"\"Test numeric string conversion\"\"\"\n",
    "    \n",
    "    df = pl.DataFrame({\n",
    "        'amount': ['123.45', '678.90'],\n",
    "        'count': ['10', '20']\n",
    "    })\n",
    "    \n",
    "    df = apply_schema(df, {\n",
    "        'amount': pl.Float64,\n",
    "        'count': pl.Int64\n",
    "    })\n",
    "    \n",
    "    assert df.schema['amount'] == pl.Float64\n",
    "    assert df.schema['count'] == pl.Int64\n",
    "    assert df['amount'].to_list() == [123.45, 678.90]\n",
    "    assert df['count'].to_list() == [10, 20]\n",
    "    print(\"✅ Test passed: Apply schema (Numeric)\")\n",
    "\n",
    "test_apply_schema_numeric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc38da9",
   "metadata": {},
   "source": [
    "## Test Row Count Return Value\n",
    "\n",
    "`map_and_upsert` now returns the actual number of rows affected by the database upsert operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75da49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test passed: map_and_upsert returns row count (3)\n"
     ]
    }
   ],
   "source": [
    "def test_map_and_upsert_returns_row_count():\n",
    "    \"\"\"Test that map_and_upsert returns the number of rows affected\"\"\"\n",
    "    \n",
    "    db_uri = 'sqlite:///file:memdb_count?mode=memory&cache=shared&uri=true'\n",
    "    engine = create_engine(db_uri)\n",
    "    \n",
    "    # Create target table\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS items\"))\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE items (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                name TEXT\n",
    "            )\n",
    "        \"\"\"))\n",
    "        conn.commit()\n",
    "    \n",
    "    # Prepare test data\n",
    "    json_data = [\n",
    "        {'id': 1, 'name': 'Item A'},\n",
    "        {'id': 2, 'name': 'Item B'},\n",
    "        {'id': 3, 'name': 'Item C'}\n",
    "    ]\n",
    "    df = pl.DataFrame(json_data)\n",
    "    \n",
    "    # Execute upsert and capture return value\n",
    "    rows_affected = map_and_upsert(\n",
    "        df=df,\n",
    "        table_name='items',\n",
    "        key_col='id',\n",
    "        db_uri=db_uri\n",
    "    )\n",
    "    \n",
    "    # Verify return value is an integer representing rows affected\n",
    "    assert isinstance(rows_affected, int), f\"Expected int, got {type(rows_affected)}\"\n",
    "    assert rows_affected == 3, f\"Expected 3 rows affected, got {rows_affected}\"\n",
    "    \n",
    "    print(f\"✅ Test passed: map_and_upsert returns row count ({rows_affected})\")\n",
    "\n",
    "test_map_and_upsert_returns_row_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8099b",
   "metadata": {},
   "source": [
    "## Test type_map Parameter\n",
    "\n",
    "`map_and_upsert` now accepts a `type_map` parameter for explicit column type casting before writing to the database. This prevents type mismatch errors when Polars infers incorrect types from nullable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae33035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test passed: map_and_upsert with type_map (3 rows)\n"
     ]
    }
   ],
   "source": [
    "def test_map_and_upsert_with_type_map():\n",
    "    \"\"\"Test type_map parameter for explicit column casting\"\"\"\n",
    "    \n",
    "    db_uri = 'sqlite:///file:memdb_typemap?mode=memory&cache=shared&uri=true'\n",
    "    engine = create_engine(db_uri)\n",
    "    \n",
    "    # Create target table with INTEGER columns\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS accounts\"))\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE accounts (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                balance_current INTEGER,\n",
    "                balance_available INTEGER\n",
    "            )\n",
    "        \"\"\"))\n",
    "        conn.commit()\n",
    "    \n",
    "    # Prepare test data with None values (Polars may infer as String)\n",
    "    json_data = [\n",
    "        {'id': 1, 'balance_current': 1000, 'balance_available': 800},\n",
    "        {'id': 2, 'balance_current': None, 'balance_available': 500},  # None value\n",
    "        {'id': 3, 'balance_current': 2000, 'balance_available': None}   # None value\n",
    "    ]\n",
    "    df = pl.DataFrame(json_data)\n",
    "    \n",
    "    # Use type_map to ensure correct types\n",
    "    rows_affected = map_and_upsert(\n",
    "        df=df,\n",
    "        table_name='accounts',\n",
    "        key_col='id',\n",
    "        db_uri=db_uri,\n",
    "        type_map={\n",
    "            'balance_current': pl.Int64,\n",
    "            'balance_available': pl.Int64\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Verify data was inserted correctly\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT * FROM accounts ORDER BY id\"))\n",
    "        rows = result.fetchall()\n",
    "        \n",
    "        assert len(rows) == 3\n",
    "        assert rows[0] == (1, 1000, 800)\n",
    "        assert rows[1] == (2, None, 500)  # None preserved\n",
    "        assert rows[2] == (3, 2000, None)  # None preserved\n",
    "    \n",
    "    print(f\"✅ Test passed: map_and_upsert with type_map ({rows_affected} rows)\")\n",
    "\n",
    "test_map_and_upsert_with_type_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1ab78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Column 'nonexistent_column' in type_map not found in DataFrame, skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test passed: map_and_upsert handles invalid type_map columns gracefully\n"
     ]
    }
   ],
   "source": [
    "def test_map_and_upsert_type_map_invalid_column():\n",
    "    \"\"\"Test that invalid columns in type_map are logged as warnings and skipped gracefully\"\"\"\n",
    "    import logging\n",
    "    \n",
    "    db_uri = 'sqlite:///file:memdb_invalid?mode=memory&cache=shared&uri=true'\n",
    "    engine = create_engine(db_uri)\n",
    "    \n",
    "    # Create target table\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS products\"))\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE products (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                price REAL\n",
    "            )\n",
    "        \"\"\"))\n",
    "        conn.commit()\n",
    "    \n",
    "    # Prepare test data\n",
    "    json_data = [\n",
    "        {'id': 1, 'price': 9.99},\n",
    "        {'id': 2, 'price': 19.99}\n",
    "    ]\n",
    "    df = pl.DataFrame(json_data)\n",
    "    \n",
    "    # type_map includes a column that doesn't exist in DataFrame\n",
    "    # This should log a warning but not raise an error\n",
    "    rows_affected = map_and_upsert(\n",
    "        df=df,\n",
    "        table_name='products',\n",
    "        key_col='id',\n",
    "        db_uri=db_uri,\n",
    "        type_map={\n",
    "            'price': pl.Float64,\n",
    "            'nonexistent_column': pl.Int64  # Should be skipped with warning\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Verify data was still inserted correctly\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT * FROM products ORDER BY id\"))\n",
    "        rows = result.fetchall()\n",
    "        \n",
    "        assert len(rows) == 2\n",
    "        assert rows_affected == 2\n",
    "    \n",
    "    print(f\"✅ Test passed: map_and_upsert handles invalid type_map columns gracefully\")\n",
    "\n",
    "test_map_and_upsert_type_map_invalid_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e8c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
