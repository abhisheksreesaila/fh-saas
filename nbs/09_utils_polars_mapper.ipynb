{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils_polars_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from __future__ import annotations\n",
    "import polars as pl\n",
    "from sqlalchemy import create_engine, text\n",
    "from typing import Dict, List, Optional\n",
    "import uuid\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e349e",
   "metadata": {},
   "source": [
    "## Polars Mapping & Database Write\n",
    "\n",
    "High-performance JSON-to-database pipeline using Polars for vectorized transformations and staging tables for bulk upserts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def map_and_upsert(\n",
    "    df: pl.DataFrame, # The raw Polars DataFrame from JSON\n",
    "    table_name: str, # Target database table name\n",
    "    key_col: str, # Primary key column for conflict resolution\n",
    "    db_uri: str, # SQLAlchemy connection string (e.g., 'sqlite:///db.db' or 'postgresql://...')\n",
    "    column_map: dict = None, # Optional rename map {json_key: db_col}\n",
    "    unnest_cols: list[str] = None # List of Struct columns to flatten\n",
    "):\n",
    "    \"\"\"\n",
    "    Map JSON data to database columns and upsert using staging table pattern.\n",
    "    \n",
    "    Performance Strategy (Staging Table Pattern):\n",
    "    1. Write to temporary staging table (fast bulk insert)\n",
    "    2. Execute SQL INSERT ... ON CONFLICT for upsert (database-native, vectorized)\n",
    "    3. Drop staging table (cleanup)\n",
    "    \n",
    "    This is 10-100x faster than row-by-row upserts for large datasets.\n",
    "    \n",
    "    Args:\n",
    "        df: Polars DataFrame with JSON data\n",
    "        table_name: Target table (must already exist)\n",
    "        key_col: Primary key for ON CONFLICT resolution\n",
    "        db_uri: Database connection string\n",
    "        column_map: Optional column renaming {json_col: db_col}\n",
    "        unnest_cols: Optional list of nested columns to flatten\n",
    "    \n",
    "    Example:\n",
    "        ```python\n",
    "        import polars as pl\n",
    "        \n",
    "        # JSON from API\n",
    "        json_data = [\n",
    "            {'user_id_val': 1, 'ABC_1': 'Alice', 'extra': 'ignore'},\n",
    "            {'user_id_val': 2, 'ABC_1': 'Bob', 'extra': 'ignore'}\n",
    "        ]\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pl.DataFrame(json_data)\n",
    "        \n",
    "        # Map and upsert\n",
    "        map_and_upsert(\n",
    "            df=df,\n",
    "            table_name='users',\n",
    "            key_col='user_id',\n",
    "            db_uri='sqlite:///app.db',\n",
    "            column_map={'user_id_val': 'user_id', 'ABC_1': 'name'}\n",
    "        )\n",
    "        ```\n",
    "    \"\"\"\n",
    "    # Step 1: Rename columns if mapping provided\n",
    "    if column_map:\n",
    "        df = df.rename(column_map)\n",
    "        logger.info(f\"Renamed columns: {column_map}\")\n",
    "    \n",
    "    # Step 2: Flatten nested columns if specified\n",
    "    if unnest_cols:\n",
    "        for col in unnest_cols:\n",
    "            if col in df.columns:\n",
    "                df = df.unnest(col)\n",
    "                logger.info(f\"Unnested column: {col}\")\n",
    "    \n",
    "    # Step 3: Select only columns that exist in target table (drop extras)\n",
    "    # This prevents errors from extra JSON fields\n",
    "    engine = create_engine(db_uri)\n",
    "    \n",
    "    # Get target table columns\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(f\"SELECT * FROM {table_name} LIMIT 0\"))\n",
    "        target_columns = list(result.keys())\n",
    "    \n",
    "    # Filter DataFrame to only target columns\n",
    "    available_cols = [col for col in target_columns if col in df.columns]\n",
    "    df = df.select(available_cols)\n",
    "    logger.info(f\"Selected columns for {table_name}: {available_cols}\")\n",
    "    \n",
    "    # Step 4: Generate unique staging table name\n",
    "    staging_table = f\"staging_{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    try:\n",
    "        # Step 5: Write to staging table (fast bulk insert)\n",
    "        df.write_database(\n",
    "            table_name=staging_table,\n",
    "            connection=db_uri,\n",
    "            if_table_exists='replace'\n",
    "        )\n",
    "        logger.info(f\"Wrote {len(df)} rows to staging table {staging_table}\")\n",
    "        \n",
    "        # Step 6: Determine database type for dialect-specific SQL\n",
    "        is_sqlite = 'sqlite' in db_uri.lower()\n",
    "        \n",
    "        # Step 7: Execute upsert from staging to target\n",
    "        with engine.connect() as conn:\n",
    "            if is_sqlite:\n",
    "                # SQLite: INSERT OR REPLACE\n",
    "                cols_str = ', '.join(available_cols)\n",
    "                upsert_sql = f\"\"\"\n",
    "                    INSERT OR REPLACE INTO {table_name} ({cols_str})\n",
    "                    SELECT {cols_str} FROM {staging_table}\n",
    "                \"\"\"\n",
    "            else:\n",
    "                # PostgreSQL: INSERT ... ON CONFLICT DO UPDATE\n",
    "                cols_str = ', '.join(available_cols)\n",
    "                update_cols = [col for col in available_cols if col != key_col]\n",
    "                update_set = ', '.join([f\"{col} = EXCLUDED.{col}\" for col in update_cols])\n",
    "                \n",
    "                upsert_sql = f\"\"\"\n",
    "                    INSERT INTO {table_name} ({cols_str})\n",
    "                    SELECT {cols_str} FROM {staging_table}\n",
    "                    ON CONFLICT ({key_col}) DO UPDATE SET {update_set}\n",
    "                \"\"\"\n",
    "            \n",
    "            conn.execute(text(upsert_sql))\n",
    "            conn.commit()\n",
    "            logger.info(f\"Upserted {len(df)} rows into {table_name}\")\n",
    "    \n",
    "    finally:\n",
    "        # Step 8: Cleanup - drop staging table\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(f\"DROP TABLE IF EXISTS {staging_table}\"))\n",
    "            conn.commit()\n",
    "            logger.info(f\"Dropped staging table {staging_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d97481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/abhisheksreesaila/fh-saas/blob/main/fh_saas/utils_polars_mapper.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### map_and_upsert\n",
       "\n",
       ">      map_and_upsert (df:polars.dataframe.frame.DataFrame, table_name:str,\n",
       ">                      key_col:str, db_uri:str, column_map:dict=None,\n",
       ">                      unnest_cols:list[str]=None)\n",
       "\n",
       "*Map JSON data to database columns and upsert using staging table pattern.*\n",
       "\n",
       "Performance Strategy (Staging Table Pattern):\n",
       "1. Write to temporary staging table (fast bulk insert)\n",
       "2. Execute SQL INSERT ... ON CONFLICT for upsert (database-native, vectorized)\n",
       "3. Drop staging table (cleanup)\n",
       "\n",
       "This is 10-100x faster than row-by-row upserts for large datasets.\n",
       "\n",
       "Args:\n",
       "    df: Polars DataFrame with JSON data\n",
       "    table_name: Target table (must already exist)\n",
       "    key_col: Primary key for ON CONFLICT resolution\n",
       "    db_uri: Database connection string\n",
       "    column_map: Optional column renaming {json_col: db_col}\n",
       "    unnest_cols: Optional list of nested columns to flatten\n",
       "\n",
       "Example:\n",
       "    ```python\n",
       "    import polars as pl\n",
       "\n",
       "    # JSON from API\n",
       "    json_data = [\n",
       "        {'user_id_val': 1, 'ABC_1': 'Alice', 'extra': 'ignore'},\n",
       "        {'user_id_val': 2, 'ABC_1': 'Bob', 'extra': 'ignore'}\n",
       "    ]\n",
       "\n",
       "    # Convert to DataFrame\n",
       "    df = pl.DataFrame(json_data)\n",
       "\n",
       "    # Map and upsert\n",
       "    map_and_upsert(\n",
       "        df=df,\n",
       "        table_name='users',\n",
       "        key_col='user_id',\n",
       "        db_uri='sqlite:///app.db',\n",
       "        column_map={'user_id_val': 'user_id', 'ABC_1': 'name'}\n",
       "    )\n",
       "    ```\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DataFrame |  | The raw Polars DataFrame from JSON |\n",
       "| table_name | str |  | Target database table name |\n",
       "| key_col | str |  | Primary key column for conflict resolution |\n",
       "| db_uri | str |  | SQLAlchemy connection string (e.g., 'sqlite:///db.db' or 'postgresql://...') |\n",
       "| column_map | dict | None | Optional rename map {json_key: db_col} |\n",
       "| unnest_cols | list | None | List of Struct columns to flatten |"
      ],
      "text/plain": [
       ">      map_and_upsert (df:polars.dataframe.frame.DataFrame, table_name:str,\n",
       ">                      key_col:str, db_uri:str, column_map:dict=None,\n",
       ">                      unnest_cols:list[str]=None)\n",
       "\n",
       "*Map JSON data to database columns and upsert using staging table pattern.*\n",
       "\n",
       "Performance Strategy (Staging Table Pattern):\n",
       "1. Write to temporary staging table (fast bulk insert)\n",
       "2. Execute SQL INSERT ... ON CONFLICT for upsert (database-native, vectorized)\n",
       "3. Drop staging table (cleanup)\n",
       "\n",
       "This is 10-100x faster than row-by-row upserts for large datasets.\n",
       "\n",
       "Args:\n",
       "    df: Polars DataFrame with JSON data\n",
       "    table_name: Target table (must already exist)\n",
       "    key_col: Primary key for ON CONFLICT resolution\n",
       "    db_uri: Database connection string\n",
       "    column_map: Optional column renaming {json_col: db_col}\n",
       "    unnest_cols: Optional list of nested columns to flatten\n",
       "\n",
       "Example:\n",
       "    ```python\n",
       "    import polars as pl\n",
       "\n",
       "    # JSON from API\n",
       "    json_data = [\n",
       "        {'user_id_val': 1, 'ABC_1': 'Alice', 'extra': 'ignore'},\n",
       "        {'user_id_val': 2, 'ABC_1': 'Bob', 'extra': 'ignore'}\n",
       "    ]\n",
       "\n",
       "    # Convert to DataFrame\n",
       "    df = pl.DataFrame(json_data)\n",
       "\n",
       "    # Map and upsert\n",
       "    map_and_upsert(\n",
       "        df=df,\n",
       "        table_name='users',\n",
       "        key_col='user_id',\n",
       "        db_uri='sqlite:///app.db',\n",
       "        column_map={'user_id_val': 'user_id', 'ABC_1': 'name'}\n",
       "    )\n",
       "    ```\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DataFrame |  | The raw Polars DataFrame from JSON |\n",
       "| table_name | str |  | Target database table name |\n",
       "| key_col | str |  | Primary key column for conflict resolution |\n",
       "| db_uri | str |  | SQLAlchemy connection string (e.g., 'sqlite:///db.db' or 'postgresql://...') |\n",
       "| column_map | dict | None | Optional rename map {json_key: db_col} |\n",
       "| unnest_cols | list | None | List of Struct columns to flatten |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(map_and_upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d11fc",
   "metadata": {},
   "source": [
    "## Helper: Apply Schema Transformations\n",
    "\n",
    "Type conversions for common API data formats (ISO dates, boolean strings, numeric strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def apply_schema(\n",
    "    df: pl.DataFrame, # Input DataFrame\n",
    "    type_map: dict # Column name -> Polars dtype (e.g., {'created_at': pl.Date, 'is_active': pl.Boolean})\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply explicit type conversions to DataFrame columns.\n",
    "    \n",
    "    Args:\n",
    "        df: Polars DataFrame\n",
    "        type_map: Dict mapping column names to Polars dtypes\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with converted types\n",
    "    \n",
    "    Example:\n",
    "        ```python\n",
    "        df = pl.DataFrame({\n",
    "            'created_at': ['2024-01-15', '2024-01-16'],\n",
    "            'is_active': ['true', 'false'],\n",
    "            'amount': ['123.45', '678.90']\n",
    "        })\n",
    "        \n",
    "        df = apply_schema(df, {\n",
    "            'created_at': pl.Date,\n",
    "            'is_active': pl.Boolean,\n",
    "            'amount': pl.Float64\n",
    "        })\n",
    "        ```\n",
    "    \"\"\"\n",
    "    conversions = []\n",
    "    \n",
    "    for col_name, dtype in type_map.items():\n",
    "        if col_name not in df.columns:\n",
    "            logger.warning(f\"Column {col_name} not found in DataFrame, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Handle different type conversions\n",
    "        if dtype == pl.Date:\n",
    "            conversions.append(pl.col(col_name).str.strptime(pl.Date, \"%Y-%m-%d\").alias(col_name))\n",
    "        elif dtype == pl.Datetime:\n",
    "            conversions.append(pl.col(col_name).str.strptime(pl.Datetime).alias(col_name))\n",
    "        elif dtype == pl.Boolean:\n",
    "            # Handle \"true\"/\"false\" strings\n",
    "            conversions.append(\n",
    "                pl.col(col_name).str.to_lowercase().eq(\"true\").alias(col_name)\n",
    "            )\n",
    "        else:\n",
    "            # Cast to specified type (works for numeric types)\n",
    "            conversions.append(pl.col(col_name).cast(dtype).alias(col_name))\n",
    "    \n",
    "    if conversions:\n",
    "        df = df.with_columns(conversions)\n",
    "        logger.info(f\"Applied schema conversions to {len(type_map)} columns\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89576df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/abhisheksreesaila/fh-saas/blob/main/fh_saas/utils_polars_mapper.py#L140){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### apply_schema\n",
       "\n",
       ">      apply_schema (df:polars.dataframe.frame.DataFrame, type_map:dict)\n",
       "\n",
       "*Apply explicit type conversions to DataFrame columns.*\n",
       "\n",
       "Args:\n",
       "    df: Polars DataFrame\n",
       "    type_map: Dict mapping column names to Polars dtypes\n",
       "\n",
       "Returns:\n",
       "    DataFrame with converted types\n",
       "\n",
       "Example:\n",
       "    ```python\n",
       "    df = pl.DataFrame({\n",
       "        'created_at': ['2024-01-15', '2024-01-16'],\n",
       "        'is_active': ['true', 'false'],\n",
       "        'amount': ['123.45', '678.90']\n",
       "    })\n",
       "\n",
       "    df = apply_schema(df, {\n",
       "        'created_at': pl.Date,\n",
       "        'is_active': pl.Boolean,\n",
       "        'amount': pl.Float64\n",
       "    })\n",
       "    ```\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| df | DataFrame | Input DataFrame |\n",
       "| type_map | dict | Column name -> Polars dtype (e.g., {'created_at': pl.Date, 'is_active': pl.Boolean}) |\n",
       "| **Returns** | **DataFrame** |  |"
      ],
      "text/plain": [
       ">      apply_schema (df:polars.dataframe.frame.DataFrame, type_map:dict)\n",
       "\n",
       "*Apply explicit type conversions to DataFrame columns.*\n",
       "\n",
       "Args:\n",
       "    df: Polars DataFrame\n",
       "    type_map: Dict mapping column names to Polars dtypes\n",
       "\n",
       "Returns:\n",
       "    DataFrame with converted types\n",
       "\n",
       "Example:\n",
       "    ```python\n",
       "    df = pl.DataFrame({\n",
       "        'created_at': ['2024-01-15', '2024-01-16'],\n",
       "        'is_active': ['true', 'false'],\n",
       "        'amount': ['123.45', '678.90']\n",
       "    })\n",
       "\n",
       "    df = apply_schema(df, {\n",
       "        'created_at': pl.Date,\n",
       "        'is_active': pl.Boolean,\n",
       "        'amount': pl.Float64\n",
       "    })\n",
       "    ```\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| df | DataFrame | Input DataFrame |\n",
       "| type_map | dict | Column name -> Polars dtype (e.g., {'created_at': pl.Date, 'is_active': pl.Boolean}) |\n",
       "| **Returns** | **DataFrame** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(apply_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import nbdev as nb\n",
    "nb.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
