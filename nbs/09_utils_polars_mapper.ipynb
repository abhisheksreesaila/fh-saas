{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils_polars_mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094886ed",
   "metadata": {},
   "source": [
    "# ðŸ»â€â„ï¸ Data Transforms\n",
    "\n",
    "> High-performance JSON-to-database pipeline using Polars vectorized transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from __future__ import annotations\n",
    "import polars as pl\n",
    "from sqlalchemy import create_engine, text\n",
    "from typing import Dict, List, Optional\n",
    "import uuid\n",
    "import logging\n",
    "from nbdev.showdoc import show_doc\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88a925",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Overview\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `map_and_upsert` | Bulk JSONâ†’DB upsert via staging table |\n",
    "| `apply_schema` | Type conversions (dates, booleans, numbers) |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ—ï¸ Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    Staging Table Pattern                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. df.write_database(staging_table) â†’ fast bulk INSERT        â”‚\n",
    "â”‚  2. INSERT ... ON CONFLICT SELECT FROM staging â†’ vectorized    â”‚\n",
    "â”‚  3. DROP TABLE staging â†’ cleanup                                â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  âš¡ 10-100x faster than row-by-row upserts                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e349e",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Map & Upsert\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `df` | Polars DataFrame from JSON |\n",
    "| `table_name` | Target database table (must exist) |\n",
    "| `key_col` | Primary key for ON CONFLICT resolution |\n",
    "| `db_uri` | Database connection string |\n",
    "| `column_map` | Optional rename map `{json_col: db_col}` |\n",
    "| `unnest_cols` | Optional list of nested columns to flatten |\n",
    "| `type_map` | Optional type casting map `{col: pl.DataType}` |\n",
    "\n",
    "**Returns:** `int` - Number of rows affected by the upsert operation\n",
    "\n",
    "**Staging Table Pattern (10-100x faster than row-by-row):**\n",
    "1. Write to temporary staging table (fast bulk insert)\n",
    "2. Execute `INSERT ... ON CONFLICT` (database-native, vectorized)\n",
    "3. Drop staging table (cleanup)\n",
    "\n",
    "**Type Casting with `type_map`:**\n",
    "\n",
    "When API data contains `None` values, Polars may infer incorrect types (e.g., `String` \n",
    "instead of `Int64`). This causes PostgreSQL type mismatch errors like:\n",
    "\n",
    "```\n",
    "column \"balance\" is of type integer but expression is of type text\n",
    "```\n",
    "\n",
    "Use `type_map` to explicitly cast columns before writing:\n",
    "\n",
    "```python\n",
    "rows = map_and_upsert(\n",
    "    df=df,\n",
    "    table_name='accounts',\n",
    "    key_col='id',\n",
    "    db_uri=db_uri,\n",
    "    type_map={\n",
    "        'balance_current': pl.Int64,\n",
    "        'balance_available': pl.Int64,\n",
    "        'balance_limit': pl.Int64\n",
    "    }\n",
    ")\n",
    "print(f\"Upserted {rows} rows\")\n",
    "```\n",
    "\n",
    "**Basic Example:**\n",
    "\n",
    "```python\n",
    "import polars as pl\n",
    "\n",
    "# JSON from API\n",
    "json_data = [\n",
    "    {'user_id_val': 1, 'ABC_1': 'Alice', 'extra': 'ignore'},\n",
    "    {'user_id_val': 2, 'ABC_1': 'Bob', 'extra': 'ignore'}\n",
    "]\n",
    "\n",
    "df = pl.DataFrame(json_data)\n",
    "\n",
    "rows_affected = map_and_upsert(\n",
    "    df=df,\n",
    "    table_name='users',\n",
    "    key_col='user_id',\n",
    "    db_uri='sqlite:///app.db',\n",
    "    column_map={'user_id_val': 'user_id', 'ABC_1': 'name'}\n",
    ")\n",
    "print(f\"Upserted {rows_affected} users\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def map_and_upsert(\n",
    "    df: pl.DataFrame, # The raw Polars DataFrame from JSON\n",
    "    table_name: str, # Target database table name\n",
    "    key_col: str, # Primary key column for conflict resolution\n",
    "    db_uri: str, # SQLAlchemy connection string (e.g., 'sqlite:///db.db' or 'postgresql://...')\n",
    "    column_map: dict = None, # Optional rename map {json_key: db_col}\n",
    "    unnest_cols: list[str] = None, # List of Struct columns to flatten\n",
    "    type_map: dict = None # Optional type casting map {col_name: pl.DataType}\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Map JSON data to database columns and upsert using staging table pattern.\n",
    "    \n",
    "    **Returns:** Number of rows affected by the upsert operation\n",
    "    \n",
    "    **Type Casting:**\n",
    "    When columns have `None` values, Polars may infer incorrect types (e.g., String\n",
    "    instead of Int64). This causes PostgreSQL type mismatch errors. Use `type_map`\n",
    "    to explicitly cast columns before writing to the database.\n",
    "    \"\"\"\n",
    "    # Step 1: Rename columns if mapping provided\n",
    "    if column_map:\n",
    "        df = df.rename(column_map)\n",
    "        logger.info(f\"Renamed columns: {column_map}\")\n",
    "    \n",
    "    # Step 2: Flatten nested columns if specified\n",
    "    if unnest_cols:\n",
    "        for col in unnest_cols:\n",
    "            if col in df.columns:\n",
    "                df = df.unnest(col)\n",
    "                logger.info(f\"Unnested column: {col}\")\n",
    "    \n",
    "    # Step 3: Apply type casting if specified\n",
    "    if type_map:\n",
    "        cast_count = 0\n",
    "        for col_name, dtype in type_map.items():\n",
    "            if col_name not in df.columns:\n",
    "                logger.warning(f\"Column '{col_name}' in type_map not found in DataFrame, skipping\")\n",
    "                continue\n",
    "            try:\n",
    "                df = df.with_columns(pl.col(col_name).cast(dtype, strict=False))\n",
    "                cast_count += 1\n",
    "                logger.debug(f\"Cast column '{col_name}' to {dtype}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to cast column '{col_name}' to {dtype}: {e}\")\n",
    "        logger.info(f\"Applied type casting to {cast_count}/{len(type_map)} columns\")\n",
    "    \n",
    "    # Step 4: Select only columns that exist in target table (drop extras)\n",
    "    # This prevents errors from extra JSON fields\n",
    "    engine = create_engine(db_uri)\n",
    "    \n",
    "    # Get target table columns\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(f\"SELECT * FROM {table_name} LIMIT 0\"))\n",
    "        target_columns = list(result.keys())\n",
    "    \n",
    "    # Filter DataFrame to only target columns\n",
    "    available_cols = [col for col in target_columns if col in df.columns]\n",
    "    df = df.select(available_cols)\n",
    "    logger.info(f\"Selected columns for {table_name}: {available_cols}\")\n",
    "    \n",
    "    # Step 5: Generate unique staging table name\n",
    "    staging_table = f\"staging_{uuid.uuid4().hex[:8]}\"\n",
    "    rows_affected = 0\n",
    "    \n",
    "    try:\n",
    "        # Step 6: Write to staging table (fast bulk insert)\n",
    "        df.write_database(\n",
    "            table_name=staging_table,\n",
    "            connection=db_uri,\n",
    "            if_table_exists='replace'\n",
    "        )\n",
    "        logger.info(f\"Wrote {len(df)} rows to staging table {staging_table}\")\n",
    "        \n",
    "        # Step 7: Determine database type for dialect-specific SQL\n",
    "        is_sqlite = 'sqlite' in db_uri.lower()\n",
    "        \n",
    "        # Step 8: Execute upsert from staging to target\n",
    "        with engine.connect() as conn:\n",
    "            if is_sqlite:\n",
    "                # SQLite: INSERT OR REPLACE\n",
    "                cols_str = ', '.join(available_cols)\n",
    "                upsert_sql = f\"\"\"\n",
    "                    INSERT OR REPLACE INTO {table_name} ({cols_str})\n",
    "                    SELECT {cols_str} FROM {staging_table}\n",
    "                \"\"\"\n",
    "            else:\n",
    "                # PostgreSQL: INSERT ... ON CONFLICT DO UPDATE\n",
    "                cols_str = ', '.join(available_cols)\n",
    "                update_cols = [col for col in available_cols if col != key_col]\n",
    "                update_set = ', '.join([f\"{col} = EXCLUDED.{col}\" for col in update_cols])\n",
    "                \n",
    "                upsert_sql = f\"\"\"\n",
    "                    INSERT INTO {table_name} ({cols_str})\n",
    "                    SELECT {cols_str} FROM {staging_table}\n",
    "                    ON CONFLICT ({key_col}) DO UPDATE SET {update_set}\n",
    "                \"\"\"\n",
    "            \n",
    "            result = conn.execute(text(upsert_sql))\n",
    "            rows_affected = result.rowcount if result.rowcount >= 0 else len(df)\n",
    "            conn.commit()\n",
    "            logger.info(f\"Upserted {rows_affected} rows into {table_name}\")\n",
    "    \n",
    "    finally:\n",
    "        # Step 9: Cleanup - drop staging table\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(f\"DROP TABLE IF EXISTS {staging_table}\"))\n",
    "            conn.commit()\n",
    "            logger.debug(f\"Dropped staging table {staging_table}\")\n",
    "    \n",
    "    return rows_affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d97481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/abhisheksreesaila/fh-saas/blob/main/fh_saas/utils_polars_mapper.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### map_and_upsert\n",
       "\n",
       ">      map_and_upsert (df:polars.dataframe.frame.DataFrame, table_name:str,\n",
       ">                      key_col:str, db_uri:str, column_map:dict=None,\n",
       ">                      unnest_cols:list[str]=None, type_map:dict=None)\n",
       "\n",
       "*Map JSON data to database columns and upsert using staging table pattern.*\n",
       "\n",
       "**Returns:** Number of rows affected by the upsert operation\n",
       "\n",
       "**Type Casting:**\n",
       "When columns have `None` values, Polars may infer incorrect types (e.g., String\n",
       "instead of Int64). This causes PostgreSQL type mismatch errors. Use `type_map`\n",
       "to explicitly cast columns before writing to the database.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DataFrame |  | The raw Polars DataFrame from JSON |\n",
       "| table_name | str |  | Target database table name |\n",
       "| key_col | str |  | Primary key column for conflict resolution |\n",
       "| db_uri | str |  | SQLAlchemy connection string (e.g., 'sqlite:///db.db' or 'postgresql://...') |\n",
       "| column_map | dict | None | Optional rename map {json_key: db_col} |\n",
       "| unnest_cols | list | None | List of Struct columns to flatten |\n",
       "| type_map | dict | None | Optional type casting map {col_name: pl.DataType} |\n",
       "| **Returns** | **int** |  |  |"
      ],
      "text/plain": [
       ">      map_and_upsert (df:polars.dataframe.frame.DataFrame, table_name:str,\n",
       ">                      key_col:str, db_uri:str, column_map:dict=None,\n",
       ">                      unnest_cols:list[str]=None, type_map:dict=None)\n",
       "\n",
       "*Map JSON data to database columns and upsert using staging table pattern.*\n",
       "\n",
       "**Returns:** Number of rows affected by the upsert operation\n",
       "\n",
       "**Type Casting:**\n",
       "When columns have `None` values, Polars may infer incorrect types (e.g., String\n",
       "instead of Int64). This causes PostgreSQL type mismatch errors. Use `type_map`\n",
       "to explicitly cast columns before writing to the database.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| df | DataFrame |  | The raw Polars DataFrame from JSON |\n",
       "| table_name | str |  | Target database table name |\n",
       "| key_col | str |  | Primary key column for conflict resolution |\n",
       "| db_uri | str |  | SQLAlchemy connection string (e.g., 'sqlite:///db.db' or 'postgresql://...') |\n",
       "| column_map | dict | None | Optional rename map {json_key: db_col} |\n",
       "| unnest_cols | list | None | List of Struct columns to flatten |\n",
       "| type_map | dict | None | Optional type casting map {col_name: pl.DataType} |\n",
       "| **Returns** | **int** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(map_and_upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d11fc",
   "metadata": {},
   "source": [
    "## ðŸ”§ Schema Transformations\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `df` | Polars DataFrame |\n",
    "| `type_map` | Dict mapping column names to Polars dtypes |\n",
    "\n",
    "**Supported conversions:**\n",
    "\n",
    "| Type | Handling |\n",
    "|------|----------|\n",
    "| `pl.Date` | Parses `YYYY-MM-DD` strings |\n",
    "| `pl.Datetime` | Parses datetime strings |\n",
    "| `pl.Boolean` | Converts `\"true\"`/`\"false\"` strings |\n",
    "| Other | Uses `cast()` (works for numeric types) |\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "df = pl.DataFrame({\n",
    "    'created_at': ['2024-01-15', '2024-01-16'],\n",
    "    'is_active': ['true', 'false'],\n",
    "    'amount': ['123.45', '678.90']\n",
    "})\n",
    "\n",
    "df = apply_schema(df, {\n",
    "    'created_at': pl.Date,\n",
    "    'is_active': pl.Boolean,\n",
    "    'amount': pl.Float64\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def apply_schema(\n",
    "    df: pl.DataFrame, # Input DataFrame\n",
    "    type_map: dict # Column name -> Polars dtype (e.g., {'created_at': pl.Date, 'is_active': pl.Boolean})\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Apply explicit type conversions to DataFrame columns.\"\"\"\n",
    "    conversions = []\n",
    "    \n",
    "    for col_name, dtype in type_map.items():\n",
    "        if col_name not in df.columns:\n",
    "            logger.warning(f\"Column {col_name} not found in DataFrame, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Handle different type conversions\n",
    "        if dtype == pl.Date:\n",
    "            conversions.append(pl.col(col_name).str.strptime(pl.Date, \"%Y-%m-%d\").alias(col_name))\n",
    "        elif dtype == pl.Datetime:\n",
    "            conversions.append(pl.col(col_name).str.strptime(pl.Datetime).alias(col_name))\n",
    "        elif dtype == pl.Boolean:\n",
    "            # Handle \"true\"/\"false\" strings\n",
    "            conversions.append(\n",
    "                pl.col(col_name).str.to_lowercase().eq(\"true\").alias(col_name)\n",
    "            )\n",
    "        else:\n",
    "            # Cast to specified type (works for numeric types)\n",
    "            conversions.append(pl.col(col_name).cast(dtype).alias(col_name))\n",
    "    \n",
    "    if conversions:\n",
    "        df = df.with_columns(conversions)\n",
    "        logger.info(f\"Applied schema conversions to {len(type_map)} columns\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89576df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/abhisheksreesaila/fh-saas/blob/main/fh_saas/utils_polars_mapper.py#L133){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### apply_schema\n",
       "\n",
       ">      apply_schema (df:polars.dataframe.frame.DataFrame, type_map:dict)\n",
       "\n",
       "*Apply explicit type conversions to DataFrame columns.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| df | DataFrame | Input DataFrame |\n",
       "| type_map | dict | Column name -> Polars dtype (e.g., {'created_at': pl.Date, 'is_active': pl.Boolean}) |\n",
       "| **Returns** | **DataFrame** |  |"
      ],
      "text/plain": [
       ">      apply_schema (df:polars.dataframe.frame.DataFrame, type_map:dict)\n",
       "\n",
       "*Apply explicit type conversions to DataFrame columns.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| df | DataFrame | Input DataFrame |\n",
       "| type_map | dict | Column name -> Polars dtype (e.g., {'created_at': pl.Date, 'is_active': pl.Boolean}) |\n",
       "| **Returns** | **DataFrame** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(apply_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import nbdev as nb\n",
    "nb.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
